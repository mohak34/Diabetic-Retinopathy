#!/usr/bin/env python3
"""
Simple, Fast Training Test Script
Tests actual training with a single configuration - should complete in 5-10 minutes
"""

import sys
import time
import logging
import torch
from pathlib import Path

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[logging.StreamHandler(sys.stdout)]
)

logger = logging.getLogger('SimpleTrainingTest')

def test_single_config_fast():
    """Test a single hyperparameter configuration with minimal epochs"""
    
    logger.info("ğŸ§ª Testing ACTUAL training with single configuration (FAST)")
    logger.info("â±ï¸ Expected time: 5-10 minutes")
    
    start_time = time.time()
    
    try:
        # Add project root to path
        project_root = Path(__file__).parent
        sys.path.append(str(project_root))
        
        # Import required modules
        from src.training.config import Phase4Config
        from src.training.trainer import RobustPhase4Trainer
        from src.data.dataloaders import DataLoaderFactory
        from src.models.simple_multi_task_model import create_simple_multi_task_model
        
        logger.info("âœ… All modules imported successfully")
        
        # Create minimal configuration for fast testing
        config = Phase4Config()
        config.experiment_name = f"test_fast_{int(time.time())}"
        
        # Set minimal epochs for speed
        config.progressive.phase1_epochs = 1  # Just 1 epoch per phase
        config.progressive.phase2_epochs = 1
        config.progressive.phase3_epochs = 1
        
        # Small batch size to go faster
        config.hardware.batch_size = 4
        config.hardware.num_workers = 2
        
        # Optimize for speed
        config.model.backbone_name = "resnet18"  # Smaller model
        config.early_stopping.patience = 1
        
        logger.info(f"ğŸ“‹ Config: {config.progressive.phase1_epochs + config.progressive.phase2_epochs + config.progressive.phase3_epochs} total epochs")
        logger.info(f"ğŸ”§ Batch size: {config.hardware.batch_size}")
        logger.info(f"ğŸ—ï¸ Model: {config.model.backbone_name}")
        
        # Create data loaders
        logger.info("ğŸ“Š Creating data loaders...")
        factory = DataLoaderFactory()
        
        loaders = factory.create_multitask_loaders(
            processed_data_dir="dataset/processed",
            splits_dir="dataset/splits", 
            batch_size=config.hardware.batch_size,
            num_workers=config.hardware.num_workers,
            image_size=224  # Smaller image size for speed
        )
        
        train_loader = loaders['train']
        val_loader = loaders['val']
        
        logger.info(f"âœ… Data loaded: {len(train_loader)} train batches, {len(val_loader)} val batches")
        
        # Create simple model
        logger.info("ğŸ—ï¸ Creating model...")
        model_config = {
            'backbone_name': config.model.backbone_name,
            'num_classes_cls': config.model.num_classes,
            'num_classes_seg': config.model.segmentation_classes,
            'pretrained': True
        }
        
        model = create_simple_multi_task_model(model_config)
        
        # Move to device
        device = 'cuda' if torch.cuda.is_available() else 'cpu'
        model = model.to(device)
        logger.info(f"âœ… Model created and moved to {device}")
        
        # Create trainer
        logger.info("ğŸš€ Creating trainer...")
        trainer = RobustPhase4Trainer(model=model, config=config)
        logger.info("âœ… Trainer created")
        
        # Create save directory
        save_dir = Path("experiments") / config.experiment_name
        save_dir.mkdir(parents=True, exist_ok=True)
        
        # Run actual training
        logger.info("ğŸƒâ€â™‚ï¸ Starting ACTUAL training...")
        training_start = time.time()
        
        results = trainer.train(
            train_loader=train_loader,
            val_loader=val_loader,
            save_dir=str(save_dir)
        )
        
        training_time = time.time() - training_start
        logger.info(f"â±ï¸ Training completed in {training_time/60:.1f} minutes")
        
        # Check results
        if results and 'best_metrics' in results:
            metrics = results['best_metrics']
            logger.info("ğŸ‰ ACTUAL TRAINING SUCCESSFUL!")
            logger.info(f"ğŸ“Š Results:")
            logger.info(f"   âœ… Accuracy: {metrics.get('accuracy', 0):.4f}")
            logger.info(f"   âœ… Dice Score: {metrics.get('dice_score', 0):.4f}")
            logger.info(f"   âœ… Combined Score: {metrics.get('combined_score', 0):.4f}")
            logger.info(f"   âœ… F1 Score: {metrics.get('f1_score', 0):.4f}")
            
            total_time = time.time() - start_time
            logger.info(f"ğŸ• Total test time: {total_time/60:.1f} minutes")
            
            return True, metrics
        else:
            logger.error("âŒ Training completed but no metrics returned")
            return False, {}
            
    except Exception as e:
        logger.error(f"âŒ Test failed: {e}")
        import traceback
        traceback.print_exc()
        return False, {}

def main():
    """Main test function"""
    
    logger.info("=" * 60)
    logger.info("SIMPLE FAST TRAINING TEST")
    logger.info("=" * 60)
    logger.info("ğŸ¯ Goal: Verify ACTUAL training works (not simulation)")
    logger.info("â±ï¸ Expected: 5-10 minutes total")
    logger.info("ğŸ”¥ Hardware: GPU training if available")
    logger.info("=" * 60)
    
    # Check CUDA availability
    if torch.cuda.is_available():
        logger.info(f"ğŸš€ CUDA available: {torch.cuda.get_device_name()}")
        logger.info(f"ğŸ’¾ CUDA memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    else:
        logger.warning("âš ï¸ CUDA not available - using CPU (will be slower)")
    
    # Run test
    success, metrics = test_single_config_fast()
    
    # Final results
    logger.info("\n" + "=" * 60)
    logger.info("TEST RESULTS")
    logger.info("=" * 60)
    
    if success:
        logger.info("ğŸ‰ SUCCESS: ACTUAL training is working!")
        logger.info("âœ… The hyperparameter optimization system can now use real training")
        logger.info("âœ… No more simulation - real neural network training confirmed")
        
        # Performance check
        accuracy = metrics.get('accuracy', 0)
        dice = metrics.get('dice_score', 0)
        
        if accuracy > 0.7 and dice > 0.5:
            logger.info("ğŸ† Performance looks reasonable for minimal training")
        else:
            logger.warning("âš ï¸ Performance is low (expected with minimal epochs)")
        
        logger.info("\nğŸš€ Ready to run hyperparameter optimization!")
        return 0
    else:
        logger.error("âŒ FAILED: Issues with actual training implementation")
        logger.error("ğŸ”§ Need to fix training pipeline before hyperparameter optimization")
        return 1

if __name__ == "__main__":
    sys.exit(main())
